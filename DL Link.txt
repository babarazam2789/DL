Practical 01A - write a program to demonstrate following operations
1) Create vector, Matrix and tensor

import numpy as np
import tensorflow as tf
D=np.array([5,6])
H=np.array([2,4])
A=np.array([[1,2],[3,4]])
B=np.array([[2,5],[7,4]])
tensor_A=tf.constant([[1,2]],dtype=tf.int32)
tensor_B=tf.constant([[4,6]],dtype=tf.int32)

2) Multiplication of Two: Vector, Matrix and Tensor

S=D*H
print("Vector Multiplication \n", S)

C=np.dot(A,B)
print("Matrix Multiplication \n", C)

y=tensor_A*tensor_B
print("Tensor Multiplication \n", y)

3) Addition of Two: Vector,Matrix and Tensor

X=D+H
print("Vector Addition \n", X)

Y=A+B
print("Matrix Addition \n", Y)

Z=tensor_A+tensor_B
print("Tensor Addition \n", Z)

4)Multiply Matrix with vector

Q=A.dot(D)
print("Multiplication of matrix with vector \n",Q)

5)Matrix Dot product and Matrix Inverse

print("Matrix Dot product \n", np.dot(A,B))


#matrix inverse
print(np.linalg.inv(A))


Practical 1B: Performing matrix multiplication and finding eigen vectors and eigen values

print("ABC")
import tensorflow as tf
print("matrix multiplication Demo")
x=tf.constant([1,2,3,4,5,6],shape=[2,3])
print(x)
y=tf.constant([7,8,9,10,11,12],shape=[3,2])
print(y)
z=tf.matmul(x, y)
print("Product: ",z)
e_matrix_A = tf.random.uniform([2, 2], minval=3, maxval=10, dtype=tf.float32, name="matrixA")
print("Matrix A: \n{}\n\n".format(e_matrix_A))
eigen_values_A, eigen_vectors_A=tf.linalg.eigh(e_matrix_A)
print("Eigen vectors: \n{}\n\n Eigen values:\n{}\n".format(eigen_vectors_A,eigen_values_A))

Practical No:02: Solving XOR Problem using deep feed forward network.

print("ABC")
import numpy as np
import tensorflow as tf
from keras.layers import Dense
from keras.models import Sequential
#model=Sequential()
model=tf.keras.models.Sequential()
model.add(tf.keras.layers.Dense(units=2, activation='relu',input_dim=2))
model.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
print("Model Summary\n",model.summary())
print("Model Weights\n",model.get_weights())
x= np.array([[0., 0.], [0., 1.], [1., 0.], [1., 1.]])
y= np.array([0.,1.,1.,0.])
model.fit(x, y, epochs=4000, batch_size=4, verbose=0)
print("Model Processed Weights\n",model.get_weights())
print("Model Predict\n",model.predict(x,batch_size=4))

Practical No: 03 - Implementing deep neural network for performing classification task.(Diabetes dataset)


from numpy import genfromtxt
from keras.models import Sequential
from keras.layers import Dense
import pandas as pd

dataset=pd.read_csv("/content/sample_data/diabetes.csv")
print(dataset)

X = dataset.iloc[:, 0:8]
Y = dataset.iloc[:, 8]
print(X.shape, Y.shape)

model=Sequential()
model.add(Dense(12,input_dim=8,activation='relu'))
model.add(Dense(8,activation='relu'))
model.add(Dense(1,activation='sigmoid'))

model.compile(loss='binary_crossentropy' ,
optimizer='adam',metrics=['accuracy'])
model.fit(X,Y,epochs=150,batch_size=10)
_,accuracy=model.evaluate(X,Y)
print("Accuracy is :",(accuracy*100))

prediction = model.predict(X)
for i in range(5):
  print(X.iloc[i].tolist(),prediction[i],Y.iloc[i])


Practical 4A - Using deep feed forward network with two hidden layers for performing classification and predicting the class

from keras.models import Sequential
from keras.layers import Dense
from sklearn.datasets import make_blobs
from sklearn.preprocessing import MinMaxScaler
x,y=make_blobs(n_samples=100,centers=2,n_features=2,random_state=1)
scalar=MinMaxScaler()
scalar.fit(x)
x=scalar.transform(x)

model=Sequential()
model.add(Dense(4,input_dim=2,activation='relu'))
model.add(Dense(4,activation='relu'))
model.add(Dense(1,activation='sigmoid'))

model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])
model.fit(x,y,epochs=500)

Xnew,Yreal=make_blobs(n_samples=3,centers=2,n_features=2,random_state=1)
Xnew=scalar.transform(Xnew)
Ynew=model.predict(Xnew)

for i in range(len(Xnew)):
  print("X=%s,predicted=%s,Desired=%s"%(Xnew[i],Ynew[i],Yreal[i]))


Practical 4B - Using a deep field forward network with two hidden layers for performing linear regression and predicting values.

from keras.models import Sequential
from keras.layers import Dense
from sklearn.datasets import make_regression
from sklearn.preprocessing import MinMaxScaler


X,Y=make_regression(n_samples=100, n_features=2, noise=0.1, random_state=1)
ScalarX,ScalarY=MinMaxScaler(),MinMaxScaler()
ScalarX.fit(X)
ScalarY.fit(Y.reshape(100,1))

model=Sequential()
model.add(Dense(4,input_dim=2,activation='relu'))
model.add(Dense(4, activation='relu'))
model.add(Dense(1, activation='sigmoid'))

model.compile(loss='mse', optimizer='adam', metrics=['accuracy'])
model.fit(X,Y,epochs=1000,batch_size=10)

Xnew,a=make_regression(n_samples=3, n_features=2, noise=0.1, random_state=1)
Xnew=ScalarX.transform(Xnew)
Ynew=model.predict(Xnew)

for i in range(len(Xnew)):
  print("X=%s,Predicted=%s"%(Xnew[i],Ynew[i]))


Practical 05A - Evaluating feed forward deep network for regression using KFold cross validation (Diabetes dataset)

from sklearn.model_selection import KFold
from keras.models import Sequential
from keras.layers import Dense
import numpy as np

dataset = np.genfromtxt("/content/sample_data/diabetes.csv", delimiter=',')

#Split the data into X and Y
X=dataset[:,0:8]
Y=dataset[:,8]

Kfold=KFold(n_splits=5, shuffle=True)

model = Sequential()
model.add(Dense(12, input_dim=8, activation='relu'))
model.add(Dense(8, activation='relu'))
model.add(Dense(1, activation='sigmoid'))

model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

#Evaluate the model using KFold cross validation
results = Kfold.split(X)
for train_index, test_index in results:
  #Get the training and testing data
  X_train, X_test = X[train_index], X[test_index]
  Y_train, Y_test = Y[train_index], Y[test_index]

model.fit(X_train, Y_train, epochs=150, batch_size=10)

_, accuracy = model.evaluate(X_test, Y_test)
print('Accuracy: %.2f%%' % (accuracy * 100))


Practical 5B - Evaluating feed forward deep network for multiclass classification using KFold cross-validation


import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from keras.models import Sequential #create layers in sequential manner
from keras.layers import Dense #represent hidden layer
from sklearn.model_selection import train_test_split

df=pd.read_csv("/content/sample_data/Flower.csv")
print(df.shape)
print()
df.head(10) #displays the first five-rows of the Dataframe

df.describe()

#DATA CLEANSING
df.isnull().sum()

df.dropna(inplace=True)
df.isnull().sum()

#EXPLORATORY DATA ANALYSIS
sns.set(style="darkgrid")
sns.pairplot(df, hue="species")

corr_matrix = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width']

sns.heatmap(df[corr_matrix].corr(), annot=True)
plt.title('Correlation between values')
plt.show()

df.info()

typeofiris = df["species"].value_counts()
print(typeofiris)

plt.pie(typeofiris, labels=typeofiris.index, autopct='%1.1f%%', startangle=90, colors=['lightblue', 'lightcoral', 'lightyellow'])
plt.title('IRIS SPECIES')
plt.show()


#DATA PRE PROCESSING
#one hot encoding

y = pd.get_dummies(df["species"])
y

x = df.drop(["species"], axis=1)
x

x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.3)

x_train

y_train

#MODEL DEVELOPMENT
model = Sequential()
model.add(Dense(4,activation="linear"))
model.add(Dense(12,activation="sigmoid"))
model.add(Dense(3,activation="softmax"))


model.compile(loss="categorical_crossentropy", metrics=["accuracy"])
model.fit(x_train, y_train, epochs=25, batch_size=3)

#MODEL EVALUATION
score = model.evaluate(x_test, y_test)
print("accuracy: ",score)

df.head(10)

class_names = ["IRIS SETOSA", "IRIS VERSICOLOR", "IRIS VIRGINICA"]
p = model.predict([[5.1,3.5,1.4,0.2]])
max_index = np.argmax(p)
predicted_class = class_names[max_index]
print(p)
print("predicted Class: ", predicted_class)

Practical 06 - Implementing regularization to avoid overfitting in binary classification

from matplotlib import pyplot as plt
from sklearn.datasets import make_moons
from keras.models import Sequential
from keras.layers import Dense
from keras.regularizers import l1_l2
x,y=make_moons(n_samples=100, noise=0.2,random_state=1)
n_train=30
trainx,testx=x[:n_train,:],x[n_train:]
trainy,testy=y[:n_train],y[n_train:]
model=Sequential()
model.add(Dense(500,input_dim=2,activation='relu',kernel_regularizer=l1_l2(l1=0.001,l2=0.001)))
model.add(Dense(1, activation='sigmoid'))
model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])
history=model.fit(trainx,trainy,validation_data=(testx,testy), epochs=1000)
plt.plot(history.history['val_accuracy'],label='test')
plt.legend()
plt.show()


Practical 07 - Performing encoding and decoding of images using deep encoders

import keras
from keras import layers
from keras.datasets import mnist
encoding_dim=32
import numpy as np

input_img=keras.Input(shape=(784,))
encode=layers.Dense(encoding_dim,activation='relu')(input_img)
decode=layers.Dense(784,activation='sigmoid')(encode)
autoencoder=keras.Model(input_img,decode)

encoder=keras.Model(input_img, encode)
encode_ip=keras.Input(shape=(encoding_dim,))
decoder_layer=autoencoder.layers[-1]
decoder=keras.Model(encode_ip,decoder_layer(encode_ip))
autoencoder.compile(optimizer='adam',loss='binary_crossentropy')

(x_train,_),(x_test,_)=mnist.load_data()
x_train=x_train.astype('float32')/255
x_test=x_test.astype('float32')/255
x_train=x_train.reshape((len(x_train),np.prod(x_train.shape[1:])))
x_test=x_test.reshape((len(x_test),np.prod(x_test.shape[1:])))
print(x_train.shape)
print(x_test.shape)

autoencoder.fit(x_train,x_train, epochs=50, batch_size=256,shuffle=True, validation_data=(x_test,x_test))
encoded_img=encoder.predict(x_test)
decoded_imgs=decoder.predict(encoded_img)
import matplotlib.pyplot as plt
n=10
plt.figure(figsize=(40,4))
for i in range(10):
  ax=plt.subplot(3,20,i+1)
  plt.imshow(x_test[i].reshape(28,28))
  plt.gray()
  ax.get_xaxis().set_visible(False)
  ax.get_yaxis().set_visible(False)
  ax=plt.subplot(3,20,2*20+i-1)
  plt.imshow(decoded_imgs[i].reshape(28,28))
  plt.gray()
  ax.get_xaxis().set_visible(False)
  ax.get_yaxis().set_visible(False)
  plt.show()

Practical 08 - Implementation of CNN to predict a number from number images

from keras.datasets import mnist
from keras.utils import to_categorical
from keras.models import Sequential
from keras.layers import Dense, Conv2D, Flatten
import matplotlib.pyplot as plt

(x_train,y_train),(x_test,y_test)=mnist.load_data()
plt.imshow(x_train[0])
plt.show()

print(x_train[0].shape)
x_train=x_train.reshape(60000,28,28,1)
x_test=x_test.reshape(10000,28,28,1)

y_train=to_categorical(y_train)
y_test=to_categorical(y_test)
y_train[0]
print(y_train[0])


model=Sequential()
model.add(Conv2D(64,kernel_size=3,activation='relu', input_shape=(28,28,1)))
model.add(Conv2D(32,kernel_size=3,activation='relu'))
model.add(Flatten())
model.add(Dense(10,activation='softmax'))
model.compile(optimizer='adam',loss='categorical_crossentropy',metrics='accuracy')
model.fit(x_train,y_train,validation_data=(x_test,y_test),epochs=3)
print(model.predict(x_test[:4]))
print(y_test[:4])

	


